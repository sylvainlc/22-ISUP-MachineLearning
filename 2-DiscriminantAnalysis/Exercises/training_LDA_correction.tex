\documentclass[a4paper,10pt,fleqn]{article}

\usepackage{a4wide,amsmath,amsthm,amssymb,bbm,fancyhdr}
\usepackage{ifthen,color,enumerate,comment,dsfont,pdfsync,framed,todonotes,enumitem}

\newcommand{\titre}[1]{\textbf{\textsc{#1}}}

\RequirePackage[T1]{fontenc}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{enumitem}
\newcommand{\eqsp}{\,}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\renewcommand{\P}{\ensuremath{\operatorname{P}}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\rme}{\ensuremath{\mathrm{e}}}
\newcommand{\calH}{\ensuremath{\mathcal{H}}}
\newcommand{\xset}{\ensuremath{\mathsf{X}}}
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\Sb}{\ensuremath{\mathbb{S}}}
\newcommand{\gaus}{\ensuremath{\mathcal{N}}}
\newcommand{\HH}{\ensuremath{\mathcal{H}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\W}{\ensuremath{\mathcal{W}}}
\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\hatk}{\widehat K}
\newcommand{\f}{\varphi}
\newcommand{\Id}{\textsf{Id}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfs}{\mathbf{\Sigma}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\argmin}{\mathop{\textrm{argmin}}}
\newcommand{\argmax}{\mathop{\textrm{argmax}}}
\newcommand{\crit}{\mathop{\textrm{crit}}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\pc}{\pi_{\mathcal{C}}}


% Style
%\pagestyle{fancyplain}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
%\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\footnotesize {\sf
%MAP569 Machine Learning II, \thisyear %/ \rightmark
%}}}
%\rhead[\fancyplain{}{\footnotesize {\sf MAT4506 Introduction to machine learning, \thisyear %/ \rightmark
%}}]{\fancyplain{}{\thepage}}


\begin{document}

\noindent Machine learning \hfill ISUP - Sorbonne Universit\'e \\
 2022-2023

\noindent\hrulefill

\begin{center}
\textsc{Discriminant analysis}
\end{center}
\hrulefill

\section{Classification error}
Linear discriminant analysis assumes that the random variables $(X,Y)\in \rset^d\times\{0,1\}$ have the following distribution. For all $A\in \mathcal{B}(\mathbb{R}^d)$ and all $y\in\{0,1\}$,
\[
\bP\left(X\in A;Y=y\right) = \pi_y \int_A g_y(x) \rmd x\eqsp,
\]
where $\pi_{0}$ and $\pi_1$ are positive real numbers such that $\pi_0+\pi_{1}=1$ and $g_0$ (resp. $g_1$) is the probability density of a Gaussian random variable with mean $\mu_0\in\rset^d$ (resp. $\mu_1)$ and positive definite covariance matrix $\Sigma_0\in \rset^{d\times d}$ (resp. $\Sigma_1$).  Define the classifier $h_{*}:\mathbb{R}^d\to\{0,1\}$ by
$$
h_{*}:x \mapsto \1_{\{\pi_{1}g_{1}(x)>\pi_{0}g_{0}(x)\}}\eqsp.
$$
\begin{enumerate}
\item Give the distribution of the random variable $X$ and prove that 
\[
\bP(h_{*}(X)\neq Y)=\min_{h:\rset^d\to \{0,1\}}\left\{\bP(h(X)\neq Y)\right\}\eqsp.
\]

\vspace{.2cm}

{\em
For all $A\in \mathcal{B}(\rset^d)$,
\begin{align*}
\bP(X \in A) & = \bP (Y = 0) \bP(X \in A | Y=0) + \bP (Y = 1) \bP(X \in A | Y=1)\eqsp, \\
& = \pi_0 \int_A g_0 (x)\rmd x + \pi_1 \int_A g_1(x)\rmd x\eqsp.
\end{align*}
The probability density of the random variable $X$ is given, for all $x\in\rset^d$, by
\[
g(x) = \pi_0 g_0 (x) + \pi_1  g_1(x)\eqsp.
\]
Then, note that 
\begin{align*}
\eta(X) = \bP(Y=1|X) = \frac{\bP(X| Y=1)\eqsp \bP(Y=1)}{g(X)} =  \frac{\pi_1 g_1(X)}{\pi_0 g_0(X) + \pi_1 g_1(X)}\eqsp,
\end{align*}
and the condition $\eta(x) \leqslant 1/2$ can be rewritten as
\[
\frac{\pi_1 g_1(x)}{\pi_0 g_0(x) + \pi_1 g_1(x)} \leqslant 1/2\eqsp,
\]
that is $\pi_1 g_1(x) \leqslant \pi_0 g_0(x)$.
}
\item Assume that  $\mu_0\ne\mu_1$. Prove that when $\Sigma_0=\Sigma_1=\Sigma$, for all $x\in\rset^d$,
\[
h_{*}(x) = 1 \Leftrightarrow (\mu_{1}-\mu_{0})^\top\Sigma^{-1}\left(x-{\mu_{1}+\mu_{0}\over2}\right)>\log(\pi_{0}/\pi_{1})\eqsp.
\]
Provide a geometrical interpretation.

\vspace{.2cm}

{\em 
For all $x\in\rset^d$, 
\begin{align*}
 \pi_1 g_1(x) &> \pi_0 g_0(x) \\
&\Leftrightarrow  \log(\pi_1 g_1(x)) > \log(\pi_0 g_0(x))\eqsp,\\
&\Leftrightarrow  -{1\over 2}(x-\mu_1)^\top\Sigma^{-1}(x-\mu_1) + {1\over 2}(x-\mu_0)^\top\Sigma^{-1}(x-\mu_0) > \log(\pi_0/ \pi_1)\eqsp,\\
&\Leftrightarrow  -{1\over 2}\Big(  -\mu_1^\top\Sigma^{-1}x + \mu_1^\top\Sigma^{-1}\mu_1  - x^\top\Sigma^{-1}\mu_1  +\mu_0^\top\Sigma^{-1}x - \mu_0^\top\Sigma^{-1}\mu_0 + x^\top\Sigma^{-1}\mu_0\Big)> \log(\pi_0/ \pi_1)\eqsp,\\
&\Leftrightarrow		 x^\top\Sigma^{-1}\mu_1 - x^\top\Sigma^{-1}\mu_0 - \frac{1}{2} \mu_1^\top\Sigma^{-1}\mu_1 + \frac{1}{2} \mu_0^\top\Sigma^{-1}\mu_0  > \log(\pi_0/ \pi_1)\eqsp,\\
&\Leftrightarrow		 (\mu_1 - \mu_0)^\top \Sigma^{-1} \Big(x - \frac{\mu_1 + \mu_0}{2}\Big)  > \log(\pi_0/ \pi_1)\eqsp.
\end{align*}
Therefore, all $x\in\rset^d$ is classified according to its position with respect to an affine hyperplane orthogonal to $\Sigma^{-1}(\mu_1-\mu_{0})$.
}
\item Prove that when $\pi_{1}=\pi_{0}$, 
\[
\bP(h_{*}(X)=1|Y=0)=\Phi(-d(\mu_{1},\mu_{0})/2)\eqsp,
\] 
where $\Phi$ is the cumulative distribution function of a standard Gaussian random variable  and 
\[
d(\mu_{1},\mu_{0})^2=(\mu_{1}-\mu_{0})^T\Sigma^{-1}(\mu_{1}-\mu_{0})\eqsp.
\]

\vspace{.2cm}

{\em
Let $Z_0$ be a Gaussian random variable with mean $\mu_0$ and variance $\Sigma$. Note that 
\begin{align*}
\bP (h_*(X) = 1 | Y = 0) = \bP \Big( \underbrace{(\mu_1 - \mu_0)^\top \Sigma^{-1} (Z_0 - \frac{\mu_1 + \mu_0}{2})}_{Z} > 0 \Big)\eqsp,
\end{align*}
where, using $\delta = d(\mu_{1},\mu_{0})$,   
\begin{align*}
\bE[Z] = (\mu_1 - \mu_0)^\top \Sigma^{-1} (\frac{\mu_0 - \mu_1 }{2})= - \frac{\delta^2}{2}
\end{align*}
and 
\begin{align*}
\mathbb{V}[Z] = \mathbb{V} \Big[ (\mu_1 - \mu_0)^\top \Sigma^{-1} X \Big] = \Big( (\mu_1 - \mu_0)^\top \Sigma^{-1}\Big) \Sigma \left( \Sigma^{-1} (\mu_1 - \mu_0)\right) = \delta^2\eqsp.
\end{align*}
Hence, 
\begin{align*}
\bP (h_*(X) = 1 | Y = 0) = \bP \Big( - \frac{\delta^2}{2} + \delta \varepsilon >0 \Big)= \bP\Big(\varepsilon > \frac{\delta}{2}\Big) = \Phi \Big(-\frac{\delta}{2}\Big).
\end{align*}
}
\item Assume now that $\Sigma_{1}\neq \Sigma_{0}$. What is the nature of the frontier between $\{x\eqsp;\eqsp h_{*}(x)=1\}$ and $\{x\eqsp;\eqsp h_{*}(x)=0\}$?

\vspace{.2cm}

{\em
In this case, for all $x\in\rset^d$, 
\begin{align*}
 \pi_1 g_1(x) &> \pi_0 g_0(x) \\
&\Leftrightarrow  \log(\pi_1 g_1(x)) > \log(\pi_0 g_0(x))\eqsp,\\
&\Leftrightarrow  -{1\over 2}(x-\mu_1)^\top\Sigma_1^{-1}(x-\mu_1) + {1\over 2}(x-\mu_0)^\top\Sigma_0^{-1}(x-\mu_0) > \log(\pi_0/ \pi_1)\eqsp,\\
&\Leftrightarrow	\frac{1}{2}x^\top\Sigma_0^{-1}x  -\frac{1}{2}x^\top\Sigma_1^{-1}x +	 x^\top\Sigma_1^{-1}\mu_1 - x^\top\Sigma_0^{-1}\mu_0 - \frac{1}{2} \mu_1^\top\Sigma^{-1}\mu_1 + \frac{1}{2} \mu_0^\top\Sigma^{-1}\mu_0  > \log(\pi_0/ \pi_1)\eqsp.
\end{align*}
As the quadratic term does not vanish anymore, the frontier between $\{x\eqsp;\eqsp h_{*}(x)=1\}$ and $\{x\eqsp;\eqsp h_{*}(x)=0\}$ is a quadric.
}
\end{enumerate}

\section{Maximum likelihood estimation}
We assume that the joint distribution of $(X,Y)$ belongs to a family of distributions parametrized by a vector $\theta$ with real components. For $k\in\{-1,1\}$, write $\pi_k = \bP(Y = k)$. Assume that conditionally on the event $\{Y = k\}$, $X$ has a Gaussian distribution with mean $\mu_k \in\rset^d$ and covariance matrix $\Sigma\in \rset^{d\times d}$, whose density is denoted $g_k$. In this case, the parameter $\theta=(\pi_1, \mu_1,\mu_{-1}, \Sigma)$ belongs to the set $\Theta= [0,1] \times \rset^d \times \rset^d \times \rset^{d \times d}$. The parameter $\pi_{-1}$ is not part of the components of $\theta$ since $\pi_{-1}=1-\pi_{1}$. In this case, the parameter $\theta=(\pi_1, \mu_1,\mu_{-1}, \Sigma)$ belongs to the set $\Theta= [0,1] \times \mathbb{R}^d \times \mathbb{R}^d \times \mathbb{R}^{d \times d}$. The parameter $\pi_{-1}$ is not part of the components of $\theta$ since $\pi_{-1}=1-\pi_{1}$. 

When $\Sigma$ and $\mu_1$ and $\mu_{-1}$ are unknown, the  discriminant analysis classifier cannot be computed explicitely. Assume that  $(X_i,Y_i)_{1\leqslant i\leqslant n}$ are independent observations with the same distribution as $(X,Y)$.
\begin{enumerate}
\item Write the joint loglikelihood of the observations.

\vspace{.2cm}

{\em
The loglikelihood of these observations is given by
\begin{align*}
\log \bP_{\theta}&\left(X_{1:n},Y_{1:n}\right) \\
&=\sum_{i=1}^n \log \bP_{\theta} \left(X_{i},Y_{i}\right)\eqsp,\\
&= - \frac{nd}{2} \log(2\pi)+\sum_{i=1}^n\sum_{k\in\{-1,1\}}\1_{Y_i=k}\left(\log \pi_{k} -\frac{\log \det \Sigma}{2} - \frac{1}{2}\left(X_i - \mu_{k}\right)^\top\Sigma^{-1}\left(X_i - \mu_{k}\right)\right) \eqsp,\\
&= - \frac{nd}{2} \log(2\pi)-\frac{n}2 \log\det\Sigma + \left(\sum_{i=1}^n\1_{Y_i=1}\right)\log \pi_1 + \left(\sum_{i=1}^n\1_{Y_i=-1}\right)\log (1-\pi_{1})\\
& \hspace{1cm}-  \frac{1}{2}\sum_{i=1}^n\1_{Y_i=1}\left(X_i - \mu_{1}\right)^\top\Sigma^{-1}\left(X_i - \mu_{1}\right) -  \frac{1}{2}\sum_{i=1}^n\1_{Y_i=-1}\left(X_i - \mu_{-1}\right)^\top\Sigma^{-1}\left(X_i - \mu_{-1}\right)\eqsp.
\end{align*}
}

\item Let $M_d$ be the space of real-valued $d \times d$ symmetric positive matrices. Show that the function $\Sigma \mapsto \log \det \Sigma$ is concave on $M_d$.

\vspace{.2cm}

{\em
Let $\Sigma,\Gamma \in M_d$ and $\lambda \in [0,1]$.  Since $\Sigma^{-1/2}\Gamma\Sigma^{-1/2} \in M_d$, it is diagonalisable in some orthonormal basis and write $\mu_1,\ldots, \mu_d$ the (possibly repeated) entries of the diagonal. Note in particular that $\det (\Sigma^{-1/2}\Gamma\Sigma^{-1/2})=\prod_{i=1}^d \mu_i$. Then,
\begin{align*}
\log \det [(1-\lambda)\Sigma+\lambda \Gamma]&=\log \det [\Sigma^{1/2} \{(1-\lambda)I+\lambda \Sigma^{-1/2}\Gamma\Sigma^{-1/2}\} \Sigma^{1/2}]\\
&=\log \det \Sigma + \log \det [(1-\lambda)I+\lambda \Sigma^{-1/2}\Gamma\Sigma^{-1/2}] \nonumber \\
&=\log \det \Sigma + \sum_{i=1}^d \log(1-\lambda+\lambda \mu_i)\nonumber \\
& \geq \log \det \Sigma + \sum_{i=1}^d (1-\lambda) \underbrace{\log(1)}_{=0}+\lambda \log( \mu_i) \label{eq:diag}:= D
\end{align*}
where the last inequality follows from the concavity of the $\log$. Now, rewrite the rhs $D$ as:
\begin{align*}
D&=(1-\lambda) \log \det \Sigma + \lambda [\log \det \Sigma^{1/2}+ \log \det \Sigma^{-1/2}\Gamma\Sigma^{-1/2} + \log \det \Sigma^{1/2}] \\
&=(1-\lambda) \log \det \Sigma + \lambda \log \det \Gamma
\end{align*}
which completes the proof.
}
\item  Show that the derivative of the real valued function $\Sigma \mapsto \log\mathrm{det}(\Sigma)$ defined on $\rset^{d\times d}$ is given by:
$$
\partial_{\Sigma}\{\log\mathrm{det}(\Sigma)\}= \Sigma^{-1}\eqsp,
$$
where, for all real valued function $f$ defined on $\rset^{d\times d}$, $\partial_{\Sigma}f(\Sigma)$ denotes the $\rset^{d\times d}$ matrix such that for all $1\leqslant i,j\leqslant d$, $\{\partial_{\Sigma}f(\Sigma)\}_{i,j}$ is the partial derivative of $f$ with respect to $\Sigma_{i,j}$.

\vspace{.2cm}

{\em
Recall that for all $i \in \{1,\ldots,d\}$ we have
      $\det(\Sigma)=\sum_{k=1}^d \Sigma_{i,k} \Delta_{i,k}$ where
      $\Delta_{i,j}$ is the $(i,j)$-cofactor associated with
      $\Sigma$. For any fixed $i,j$, the component $\Sigma_{i,j}$ does not appear  anywhere in
      the decomposition $\sum_{k=1}^d \Sigma_{i,k} \Delta_{i,k}$,
      except for the term $k=j$. This implies
      $$
      \frac{\partial \log \det(\Sigma)}{\partial \Sigma_{i,j}}=
      \frac{1}{\det \Sigma}\frac{\partial  \det(\Sigma)}{\partial
        \Sigma_{i,j}}=\frac{\Delta_{i,j}}{\det  \Sigma}\,.
      $$
      Recalling the identity $\Sigma\; [\Delta_{j,i}]_{1\leq i,j \leq d}=(\det
      \Sigma)\; I_d$ so that $\Sigma^{-1}=[\Delta_{j,i}]_{1\leq i,j \leq d}^\top/\det
      \Sigma$, we finally get
      $$
(\frac{\partial \log \det(\Sigma)}{\partial \Sigma_{i,j}})_{1\leq
  i,j  \leq d}=(\Sigma^{-1})^\top=\Sigma^{-1}\,,
$$
where the last equality follows from the fact that $\Sigma$ is
symmetric.
}
\item Provide the maximum likehood estimator of $\theta$.

\vspace{.2cm}

{\em
The gradient of $\log \bP_{\theta}\left(X_{1:n},Y_{1:n}\right)$ with respect to $\theta$ is therefore given by
\begin{align*}
\frac{\partial \log \bP_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \pi_1} &= \left(\sum_{i=1}^n\1_{Y_i=1}\right)\frac{1}{\pi_1} - \left(\sum_{i=1}^n\1_{Y_i=-1}\right)\frac{1}{1-\pi_{1}}\eqsp,\\
\frac{\partial \log \bP_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \mu_1} &= \sum_{i=1}^n\1_{Y_i=1}\left(2\Sigma^{-1}X_i - 2\Sigma^{-1}\mu_{1}\right)\eqsp,\\
\frac{\partial \log \bP_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \mu_{-1}} &= \sum_{i=1}^n\1_{Y_i=-1}\left(2\Sigma^{-1}X_i - 2\Sigma^{-1}\mu_{-1}\right)\eqsp,\\
\frac{\partial \log \bP_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \Sigma^{-1}} &= \frac{n}{2}\Sigma -  \frac{1}{2}\sum_{i=1}^n\1_{Y_i=1}\left(X_i - \mu_{1}\right)\left(X_i - \mu_{1}\right)^\top -  \frac{1}{2}\sum_{i=1}^n\1_{Y_i=-1}\left(X_i - \mu_{-1}\right)\left(X_i - \mu_{-1}\right)^\top\eqsp.
\end{align*}
The maximum likelihood estimator is defined as the only parameter $\widehat \theta^n$ such that all these equations are set to 0. For $k\in\{-1,1\}$,  it is given by
\begin{align*}
\widehat \pi_k^n &= \frac{1}{n}\sum_{i=1}^n\1_{Y_i=k}\eqsp,\\
\widehat \mu_k^n &= \frac{1}{\sum_{i=1}^n\1_{Y_i=k}}\sum_{i=1}^n\1_{Y_i=k}\,X_i\eqsp,\\
\widehat\Sigma^n &= \frac{1}{n}\sum_{i=1}^n \left(X_i - \widehat \mu_{Y_i}^n\right)\left(X_i - \widehat \mu_{Y_i}^n\right)^\top\eqsp.
\end{align*}
}
\end{enumerate}



\end{document}